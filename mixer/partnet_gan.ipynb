{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "partnet_gan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv6-Z6HhJpK1"
      },
      "source": [
        "# CMPT 764 - Final Project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shmyhjM8xlF7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rzc2Ff4x5Az"
      },
      "source": [
        "!7z x gdrive/MyDrive/SFU/PartNet_Chairs_Clean.7z -ogdrive/MyDrive/SFU/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghQy4LMfJlRZ"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neAFz3fc4sGP"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "partnet_dir = \"gdrive/MyDrive/SFU/PartNet_Chairs_Clean/Chair_Parts/\"\n",
        "model_dir = \"gdrive/MyDrive/SFU/PartNet_GAN/\"\n",
        "\n",
        "chair_ids_all = np.array([f.name for f in os.scandir(partnet_dir) if f.is_dir()])\n",
        "num_chairs = chair_ids_all.shape[0]\n",
        "shuffle_idx = np.random.permutation(num_chairs)\n",
        "chair_ids_all = chair_ids_all[shuffle_idx]\n",
        "chair_ids_train, chair_ids_test, chair_ids_validate = np.array_split(chair_ids_all, [int(num_chairs * 0.8), int(num_chairs * 0.9)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytjc4y4t_CsD"
      },
      "source": [
        "import itertools\n",
        "\n",
        "\n",
        "def load_batch(chair_ids):\n",
        "    chair_arm_pts, chair_back_pts, chair_base_pts, chair_seat_pts, other_pts = [], [], [], [], []\n",
        "\n",
        "    for chair_id in chair_ids:\n",
        "        chair_arm_pts.append(parse_pts_file(os.path.join(partnet_dir, chair_id, \"chair_arm_pts.xyz\")))\n",
        "        chair_back_pts.append(parse_pts_file(os.path.join(partnet_dir, chair_id, \"chair_back_pts.xyz\")))\n",
        "        chair_base_pts.append(parse_pts_file(os.path.join(partnet_dir, chair_id, \"chair_base_pts.xyz\")))\n",
        "        chair_seat_pts.append(parse_pts_file(os.path.join(partnet_dir, chair_id, \"chair_seat_pts.xyz\")))\n",
        "        other_pts.append(parse_pts_file(os.path.join(partnet_dir, chair_id, \"other_pts.xyz\")))\n",
        "\n",
        "    return chair_arm_pts, chair_back_pts, chair_base_pts, chair_seat_pts, other_pts\n",
        "\n",
        "\n",
        "def load_batch_monolithic(chair_ids, fuse=True):\n",
        "    chair_arm_pts, chair_back_pts, chair_base_pts, chair_seat_pts, other_pts = load_batch(chair_ids)\n",
        "    chair_all_pts = []\n",
        "\n",
        "    for idx in range(len(chair_ids)):\n",
        "        chair_pts = []\n",
        "        chair_pts.extend(chair_arm_pts[idx])\n",
        "        chair_pts.extend(chair_back_pts[idx])\n",
        "        chair_pts.extend(chair_base_pts[idx])\n",
        "        chair_pts.extend(chair_seat_pts[idx])\n",
        "        chair_pts.extend(other_pts[idx])\n",
        "\n",
        "        if fuse:\n",
        "            chair_all_pts.extend(chair_pts)\n",
        "        else:\n",
        "            chair_all_pts.append(chair_pts)\n",
        "\n",
        "    return np.array(chair_all_pts)\n",
        "\n",
        "\n",
        "def parse_pts_file(file_path):\n",
        "    xyz_data = []\n",
        "\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line_str in f.read().splitlines():\n",
        "            xyz_data_raw = line_str.split()\n",
        "            x = float(xyz_data_raw[0])\n",
        "            y = float(xyz_data_raw[1])\n",
        "            z = float(xyz_data_raw[2])\n",
        "            xyz_data.append([x, y, z])\n",
        "\n",
        "    return np.array(xyz_data)\n",
        "\n",
        "\n",
        "def split_batch(chair_ids, split_size=2):\n",
        "    batch_seq = []\n",
        "\n",
        "    for chair_ids_seq in itertools.combinations(chair_ids, split_size):\n",
        "        batch_seq.append(chair_ids_seq)\n",
        "\n",
        "    return batch_seq\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDGlz5ZEJ1OA"
      },
      "source": [
        "## GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvbCGpgxRn5n"
      },
      "source": [
        "### Define Generator & Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJkRNrCIJ9qE"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "\n",
        "def generator():\n",
        "    gen = keras.models.Sequential()\n",
        "    gen.add(keras.layers.InputLayer(input_shape=(20000, 3)))\n",
        "    gen.add(keras.layers.Conv1D(filters=128, kernel_size=1, activation=\"relu\"))\n",
        "    gen.add(keras.layers.BatchNormalization())\n",
        "    gen.add(keras.layers.Conv1D(filters=512, kernel_size=1, activation=\"relu\"))\n",
        "    gen.add(keras.layers.BatchNormalization())\n",
        "    gen.add(keras.layers.Conv1D(filters=1024, kernel_size=1, activation=\"relu\"))\n",
        "    gen.add(keras.layers.BatchNormalization())\n",
        "    gen.add(keras.layers.MaxPool1D(pool_size=2))\n",
        "    gen.add(keras.layers.Conv1D(filters=512, kernel_size=1, activation=\"relu\"))\n",
        "    gen.add(keras.layers.BatchNormalization())\n",
        "    gen.add(keras.layers.Conv1D(filters=128, kernel_size=1, activation=\"relu\"))\n",
        "    gen.add(keras.layers.BatchNormalization())\n",
        "    gen.add(keras.layers.Conv1D(filters=3, kernel_size=1, activation=\"tanh\"))\n",
        "\n",
        "    return gen\n",
        "\n",
        "\n",
        "def discriminator():\n",
        "    disc = keras.models.Sequential()\n",
        "    disc.add(keras.layers.InputLayer(input_shape=(10000, 3)))\n",
        "    disc.add(keras.layers.Conv1D(filters=128, kernel_size=1, activation=\"relu\"))\n",
        "    disc.add(keras.layers.BatchNormalization())\n",
        "    disc.add(keras.layers.Conv1D(filters=512, kernel_size=1, activation=\"relu\"))\n",
        "    disc.add(keras.layers.BatchNormalization())\n",
        "    disc.add(keras.layers.Conv1D(filters=1024, kernel_size=1, activation=\"relu\"))\n",
        "    disc.add(keras.layers.BatchNormalization())\n",
        "    disc.add(keras.layers.MaxPool1D(pool_size=10000))\n",
        "    disc.add(keras.layers.Flatten())\n",
        "    disc.add(keras.layers.Dense(units=512, activation=\"relu\"))\n",
        "    disc.add(keras.layers.BatchNormalization())\n",
        "    disc.add(keras.layers.Dense(units=128, activation=\"relu\"))\n",
        "    disc.add(keras.layers.BatchNormalization())\n",
        "    disc.add(keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "    return disc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MydCVkefR9jn"
      },
      "source": [
        "### Define Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efijmw9nxgod"
      },
      "source": [
        "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "\n",
        "    return total_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HStQ--LdST-_"
      },
      "source": [
        "### Define Training Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGLx8YL2J2rL"
      },
      "source": [
        "gen = generator()\n",
        "disc = discriminator()\n",
        "\n",
        "gen_optimizer = keras.optimizers.Adam(1e-4)\n",
        "disc_optimizer = keras.optimizers.Adam(1e-4)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(chair_pts_gen, chair_pts_disc):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_chairs = gen(chair_pts_gen, training=True)\n",
        "\n",
        "        real_output = disc(chair_pts_disc, training=True)\n",
        "        fake_output = disc(generated_chairs, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gen_gradients = gen_tape.gradient(gen_loss, gen.trainable_variables)\n",
        "    disc_gradients = disc_tape.gradient(disc_loss, disc.trainable_variables)\n",
        "\n",
        "    gen_optimizer.apply_gradients(zip(gen_gradients, gen.trainable_variables))\n",
        "    disc_optimizer.apply_gradients(zip(disc_gradients, disc.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdWtng6jSFX6"
      },
      "source": [
        "### Pre-Process Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSPx9k_NE8rf"
      },
      "source": [
        "num_chairs = chair_ids_train.shape[0]\n",
        "batch_size = 4\n",
        "num_batches = num_chairs // batch_size if num_chairs % batch_size == 0 else num_chairs // batch_size + 1\n",
        "\n",
        "batch_all_gen, batch_all_disc = [], []\n",
        "batch_counter = 1\n",
        "\n",
        "for batch_idx in range(0, num_chairs, batch_size):\n",
        "    print(\"\\rPre-Processing Batch %d / %d...\" % (batch_counter, num_batches), end=\"\")\n",
        "\n",
        "    batch_chair_ids_idx = range(batch_idx, min(batch_idx + batch_size, num_chairs))\n",
        "    batch_chair_ids = chair_ids_train[batch_chair_ids_idx]\n",
        "    batch_chair_ids_split = split_batch(batch_chair_ids)\n",
        "    batch_chair_pts_gen, batch_chair_pts_disc = [], []\n",
        "\n",
        "    for batch_chair_ids_seq in batch_chair_ids_split:\n",
        "        batch_chair_pts_gen.append(load_batch_monolithic(batch_chair_ids_seq, fuse=True))\n",
        "\n",
        "    batch_chair_pts_gen = np.array(batch_chair_pts_gen)\n",
        "    batch_chair_pts_disc = np.array(load_batch_monolithic(batch_chair_ids, fuse=False))\n",
        "\n",
        "    batch_all_gen.append(batch_chair_pts_gen)\n",
        "    batch_all_disc.append(batch_chair_pts_disc)\n",
        "\n",
        "    batch_counter += 1\n",
        "\n",
        "batch_all_gen = np.array(batch_all_gen)\n",
        "batch_all_disc = np.array(batch_all_disc)\n",
        "\n",
        "print(\"\\nDONE!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNz3iJEnSea_"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1NGZnnCb_0Z"
      },
      "source": [
        "num_epochs = 20\n",
        "gen_loss_values, disc_loss_values = [], []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(\"\\nTraining Epoch %d / %d...\" % (epoch, num_epochs))\n",
        "    batch_counter = 1\n",
        "    batch_gen_loss, batch_disc_loss = 0, 0\n",
        "\n",
        "    for batch_idx in range(0, num_batches):\n",
        "        print(\"\\rBatch %d / %d...\" % (batch_counter, num_batches), end=\"\")\n",
        "\n",
        "        gen_loss, disc_loss = train_step(batch_all_gen[batch_idx], batch_all_disc[batch_idx])\n",
        "        batch_gen_loss += gen_loss\n",
        "        batch_disc_loss += disc_loss\n",
        "\n",
        "        batch_counter += 1\n",
        "\n",
        "    gen_loss_values.append(batch_gen_loss)\n",
        "    disc_loss_values.append(batch_disc_loss)\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(generator=gen, generator_optimizer=gen_optimizer, discriminator=disc, discriminator_optimizer=disc_optimizer)\n",
        "    checkpoint.save(model_dir)\n",
        "\n",
        "print(\"\\nDONE!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en_1Uh9-SibF"
      },
      "source": [
        "### Visualize Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffr1N5gBj3kM"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(range(num_epochs), gen_loss_values, label=\"Generator\")\n",
        "plt.plot(range(num_epochs), disc_loss_values, label=\"Discriminator\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"GAN Training Loss Values\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDFwl5E4Sk-t"
      },
      "source": [
        "### Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvAw8g_1ekjt"
      },
      "source": [
        "import mpl_toolkits.mplot3d.Axes3D as axes3d\n",
        "\n",
        "\n",
        "chair_ids_test_sample = chair_ids_test[0 : 2]\n",
        "print(chair_ids_test_sample)\n",
        "\n",
        "gen_pts_in = load_batch_monolithic(chair_ids_test_sample, fuse=True)\n",
        "disc_pts_in = load_batch_monolithic(chair_ids_test_sample, fuse=False)\n",
        "\n",
        "gen_pts_out = gen(np.array([gen_pts_in]))\n",
        "\n",
        "show_point_cloud(disc_pts_in[0, :, :])\n",
        "show_point_cloud(disc_pts_in[1, :, :])\n",
        "show_point_cloud(gen_pts_out[0, :, :])\n",
        "\n",
        "\n",
        "def show_point_cloud(pts):\n",
        "    fig = plt.figure()\n",
        "    ax = axes3D(fig)\n",
        "    ax.scatter(pts[:, 0], pts[:, 1], pts[:, 2])\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X87M2-V3RJKW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}